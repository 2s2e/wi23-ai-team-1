{
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab65a6a126614c4d8a09c3bb162b3d2e4f4a949753c6f0f735c7c1fe269df83b"
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Cleaning",
   "metadata": {
    "id": "AYVsepnsN7Ys"
   }
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:17:30.467255Z",
     "iopub.execute_input": "2023-03-04T21:17:30.467664Z",
     "iopub.status.idle": "2023-03-04T21:17:36.629817Z",
     "shell.execute_reply.started": "2023-03-04T21:17:30.467631Z",
     "shell.execute_reply": "2023-03-04T21:17:36.628815Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = pd.read_csv(\"/kaggle/input/dataset/train_cleaned.csv\")\n\ndata.head()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "T3ng90-PS0JD",
    "outputId": "5bab9c74-8f9f-4c7d-d1b4-e034f7b7b3be",
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2023-03-04T21:17:36.633038Z",
     "iopub.execute_input": "2023-03-04T21:17:36.634045Z",
     "iopub.status.idle": "2023-03-04T21:17:39.291839Z",
     "shell.execute_reply.started": "2023-03-04T21:17:36.634003Z",
     "shell.execute_reply": "2023-03-04T21:17:39.290747Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  explanation\\n  edits made   username hardcore ...      0   \n1  000103f0d9cfb60f  daww  matches  background colour im seemingly ...      0   \n2  000113f07ec002fd  hey man im really  trying  edit war     guy  c...      0   \n3  0001b41b1c6bb37e  cant make  real suggestions  improvement   won...      0   \n4  0001d958c54c6e35           sir   hero  chance  remember  page thats      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                  comment_text_words  \n0  ['explanation', 'edits', 'made', 'username', '...  \n1  ['daww', 'matches', 'background', 'colour', 'i...  \n2  ['hey', 'man', 'im', 'really', 'trying', 'edit...  \n3  ['cant', 'make', 'real', 'suggestions', 'impro...  \n4  ['sir', 'hero', 'chance', 'remember', 'page', ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>comment_text_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>explanation\\n  edits made   username hardcore ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['explanation', 'edits', 'made', 'username', '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>daww  matches  background colour im seemingly ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['daww', 'matches', 'background', 'colour', 'i...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>hey man im really  trying  edit war     guy  c...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['hey', 'man', 'im', 'really', 'trying', 'edit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>cant make  real suggestions  improvement   won...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['cant', 'make', 'real', 'suggestions', 'impro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>sir   hero  chance  remember  page thats</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Tokenizing Comments using TextVectorization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# finding the maximum number of words present in any given comment\nmaxlen = 0\nlongest_comment = \"\"\nfor comment in data['comment_text']:\n    length = len(comment)\n    if (length > maxlen):\n        longest_comment = comment\n    maxlen = max(maxlen, length)\n\nprint(\"Number of characters in the longest comment is\", maxlen)\nprint(\"Number of words in the longest comment is\",\n      len(longest_comment.split(\" \")) + 1)",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of characters in the longest comment is 5000\n\nNumber of words in the longest comment is 456\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\n\nvectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n\n    max_tokens=None,\n\n    # this is greater than the max words any comment has (774)\n    # the remaning spots in the output would be padded by 0s\n    output_sequence_length=800,\n\n    # converets to lowercase and skips all the punctuation\n    standardize=\"lower_and_strip_punctuation\",\n\n    # the tokens will be split at whitespaces\n    split=\"whitespace\",\n\n    # each of the tokens is represented as an integer\n    output_mode=\"int\",\n)",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "numpyArray = data[data.columns[1]].to_numpy()\nvectorize_layer.adapt(numpyArray)",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# testing\nvectorize_layer(\"hello, world!\")",
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(800,), dtype=int64, numpy=\n",
       "array([185, 161,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Creating a Tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Gets all the words into one string",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Creates the tokenizer class\ntokenizer = keras.preprocessing.text.Tokenizer()\n\n# Combines all the words into one singular string\nallWordsString = \" \".join(data.head(50)[\"comment_text\"].tolist())\nallWordsList = allWordsString.split(r\"\\\\s+\")\n\n# Updates the tokenizer with the string of all words\ntokenizer.fit_on_texts(allWordsList)\n\n# Prints word dictionary\n# print(tokenizer.word_index)\n\n# Prints length of word dictionary\nprint(len(tokenizer.word_index))\n\n# Converts text to numbers\nprint(tokenizer.texts_to_sequences([\"page\", \"im\", \"use\", \"mussolini\"]))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-26T19:53:21.214091Z",
     "iopub.execute_input": "2023-02-26T19:53:21.215398Z",
     "iopub.status.idle": "2023-02-26T19:53:21.229347Z",
     "shell.execute_reply.started": "2023-02-26T19:53:21.215338Z",
     "shell.execute_reply": "2023-02-26T19:53:21.228130Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "1077\n[[1], [2], [3], [1077]]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Text Vectorization Layer\n### Following [https://www.tensorflow.org/text/tutorials/text_classification_rnn](http://)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "NUM_ROWS = 10000\n\nMAX_LENGTH = None\nencoder = tf.keras.layers.TextVectorization(output_sequence_length=MAX_LENGTH)\nencoder.adapt(data.head(NUM_ROWS)[\"comment_text\"].tolist())\n\nvocab = np.array(encoder.get_vocabulary())\nvocab[:20]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:41:08.676437Z",
     "iopub.execute_input": "2023-03-04T21:41:08.676827Z",
     "iopub.status.idle": "2023-03-04T21:41:09.821725Z",
     "shell.execute_reply.started": "2023-03-04T21:41:08.676790Z",
     "shell.execute_reply": "2023-03-04T21:41:09.820615Z"
    },
    "trusted": true
   },
   "execution_count": 51,
   "outputs": [
    {
     "execution_count": 51,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['', '[UNK]', 'article', 'page', 'wikipedia', 'would', 'talk',\n       'like', 'one', 'please', 'dont', 'see', 'also', 'im', 'know',\n       'think', 'edit', 'people', 'use', 'articles'], dtype='<U322')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Testing the encoder\n\nEncoder removes punctuation and whitespace and forces lowercase so half the cleaning we did was useless",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "commentsToEncode = data.head(3)[\"comment_text\"]\nprint(commentsToEncode)\n\nencodedComments = encoder(commentsToEncode).numpy()\nprint(encodedComments)\n\nfor comment in encodedComments:\n    print(\" \".join(vocab[comment]))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-27T00:40:50.800600Z",
     "iopub.execute_input": "2023-02-27T00:40:50.801014Z",
     "iopub.status.idle": "2023-02-27T00:40:50.899632Z",
     "shell.execute_reply.started": "2023-02-27T00:40:50.800979Z",
     "shell.execute_reply": "2023-02-27T00:40:50.898088Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "0    explanation\\n  edits made   username hardcore ...\n1    daww  matches  background colour im seemingly ...\n2    hey man im really  trying  edit war     guy  c...\nName: comment_text, dtype: object\n[[ 120   54   76  484 1020  867 1094  148  465  483 1273 1047  476  320\n   438 1168 1104    7    8  104  258    5    2   67    3  697]\n [1211  885 1346 1267    3  661  263   29    5  955   90    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0]\n [ 355   75    3  106   62   13  472 1028  392  293  294   49  546   54\n   976    5    2  284 1301  368 1421  342    0    0    0    0]]\nexplanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired\ndaww matches background colour im seemingly stuck thanks talk january utc               \nhey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info    \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Building the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sets random seed so results are identical every time\nSEED = 1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nmodel = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=512,\n        mask_zero=True\n    ),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n             optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=[\"accuracy\"])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:41:11.650270Z",
     "iopub.execute_input": "2023-03-04T21:41:11.650643Z",
     "iopub.status.idle": "2023-03-04T21:41:13.551799Z",
     "shell.execute_reply.started": "2023-03-04T21:41:11.650612Z",
     "shell.execute_reply": "2023-03-04T21:41:13.550652Z"
    },
    "trusted": true
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n### Training the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "binaryDf = data.head(NUM_ROWS)[[\"comment_text\", \"toxic\"]]\n\nsplit_cutoff = int(0.8 * NUM_ROWS)\ntraining_data = binaryDf.iloc[:split_cutoff]\nvalidation_data = binaryDf.iloc[split_cutoff:]\n\ntraining_target = training_data.pop(\"toxic\")\nvalidation_target = validation_data.pop(\"toxic\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:41:13.553660Z",
     "iopub.execute_input": "2023-03-04T21:41:13.554019Z",
     "iopub.status.idle": "2023-03-04T21:41:13.561559Z",
     "shell.execute_reply.started": "2023-03-04T21:41:13.553982Z",
     "shell.execute_reply": "2023-03-04T21:41:13.560588Z"
    },
    "trusted": true
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# # training_dataset = tf.convert_to_tensor(training_data)\n# # validation_dataset = tf.convert_to_tensor(validation_data)\n\n# validation_target = validation_data.pop(\"toxic\")\n# validation_dataset = tf.convert_to_tensor(validation_data)\n\n# # history = model.fit(training_dataset, target, epochs=10, validation_data=(validation_dataset, validation_target))\n\n# binaryDf = data.head(NUM_ROWS)[[\"comment_text\", \"toxic\"]]\n# target = binaryDf.pop(\"toxic\")\n# dataset = tf.convert_to_tensor(binaryDf)\n\nhistory = model.fit(training_data, training_target, epochs=10, validation_data=(validation_data, validation_target))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:41:14.046677Z",
     "iopub.execute_input": "2023-03-04T21:41:14.047376Z",
     "iopub.status.idle": "2023-03-04T21:42:50.573289Z",
     "shell.execute_reply.started": "2023-03-04T21:41:14.047339Z",
     "shell.execute_reply": "2023-03-04T21:42:50.572315Z"
    },
    "trusted": true
   },
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\n250/250 [==============================] - 21s 46ms/step - loss: 0.4203 - accuracy: 0.9045 - val_loss: 0.2458 - val_accuracy: 0.9125\nEpoch 2/10\n250/250 [==============================] - 9s 36ms/step - loss: 0.1881 - accuracy: 0.9181 - val_loss: 0.1699 - val_accuracy: 0.9395\nEpoch 3/10\n250/250 [==============================] - 8s 34ms/step - loss: 0.0756 - accuracy: 0.9732 - val_loss: 0.1670 - val_accuracy: 0.9455\nEpoch 4/10\n250/250 [==============================] - 8s 32ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.2038 - val_accuracy: 0.9490\nEpoch 5/10\n250/250 [==============================] - 9s 35ms/step - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.2313 - val_accuracy: 0.9450\nEpoch 6/10\n250/250 [==============================] - 9s 35ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.2564 - val_accuracy: 0.9420\nEpoch 7/10\n250/250 [==============================] - 8s 32ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.2936 - val_accuracy: 0.9390\nEpoch 8/10\n250/250 [==============================] - 8s 32ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.3281 - val_accuracy: 0.9485\nEpoch 9/10\n250/250 [==============================] - 9s 35ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2620 - val_accuracy: 0.9495\nEpoch 10/10\n250/250 [==============================] - 8s 33ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.2651 - val_accuracy: 0.9465\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# sample_text = \"\"\n# predictions = model.predict(np.array([sample_text]))\n# predictions[0]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T01:06:55.562502Z",
     "iopub.execute_input": "2023-03-04T01:06:55.563202Z",
     "iopub.status.idle": "2023-03-04T01:06:55.589164Z",
     "shell.execute_reply.started": "2023-03-04T01:06:55.563096Z",
     "shell.execute_reply": "2023-03-04T01:06:55.588291Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Getting test data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_data = pd.read_csv(\"/kaggle/input/toxic-competition-dataset/test.csv\")\ntest_labels = pd.read_csv(\"/kaggle/input/toxic-competition-dataset/test_labels.csv\")\n\ntest_labels = test_labels.loc[test_labels[\"toxic\"] >= 0]\nmerged_df = test_labels.merge(test_data, left_on=\"id\", right_on=\"id\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:18:05.755203Z",
     "iopub.execute_input": "2023-03-04T21:18:05.755599Z",
     "iopub.status.idle": "2023-03-04T21:18:07.459721Z",
     "shell.execute_reply.started": "2023-03-04T21:18:05.755560Z",
     "shell.execute_reply": "2023-03-04T21:18:07.458700Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Tests all rows with a value of 0 or 1\n\ntest_df = merged_df[[\"comment_text\", \"toxic\"]]\ntestTarget = test_df.pop(\"toxic\")\nmodel.evaluate(test_df, testTarget)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:42:50.575964Z",
     "iopub.execute_input": "2023-03-04T21:42:50.576324Z",
     "iopub.status.idle": "2023-03-04T21:43:29.181385Z",
     "shell.execute_reply.started": "2023-03-04T21:42:50.576287Z",
     "shell.execute_reply": "2023-03-04T21:43:29.180485Z"
    },
    "trusted": true
   },
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "text": "2000/2000 [==============================] - 39s 19ms/step - loss: 0.2968 - accuracy: 0.9280\n",
     "output_type": "stream"
    },
    {
     "execution_count": 55,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.29684823751449585, 0.9280377626419067]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Tests only rows with a toxic value of 1\n\ntest_df = merged_df[[\"comment_text\", \"toxic\"]]\nnewTest_df = test_df.loc[test_df[\"toxic\"] == 1]\n\nnewTestTarget = newTest_df.pop(\"toxic\")\nmodel.evaluate(newTest_df, newTestTarget)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T21:43:29.183095Z",
     "iopub.execute_input": "2023-03-04T21:43:29.183511Z",
     "iopub.status.idle": "2023-03-04T21:43:32.586492Z",
     "shell.execute_reply.started": "2023-03-04T21:43:29.183469Z",
     "shell.execute_reply": "2023-03-04T21:43:32.585199Z"
    },
    "trusted": true
   },
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "text": "191/191 [==============================] - 3s 18ms/step - loss: 1.8954 - accuracy: 0.5660\n",
     "output_type": "stream"
    },
    {
     "execution_count": 56,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1.8954060077667236, 0.5660098791122437]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
