{"cells":[{"cell_type":"markdown","metadata":{"id":"AYVsepnsN7Ys"},"source":["# Cleaning"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T21:17:30.467664Z","iopub.status.busy":"2023-03-04T21:17:30.467255Z","iopub.status.idle":"2023-03-04T21:17:36.629817Z","shell.execute_reply":"2023-03-04T21:17:36.628815Z","shell.execute_reply.started":"2023-03-04T21:17:30.467631Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"execution":{"iopub.execute_input":"2023-03-04T21:17:36.634045Z","iopub.status.busy":"2023-03-04T21:17:36.633038Z","iopub.status.idle":"2023-03-04T21:17:39.291839Z","shell.execute_reply":"2023-03-04T21:17:39.290747Z","shell.execute_reply.started":"2023-03-04T21:17:36.634003Z"},"id":"T3ng90-PS0JD","outputId":"5bab9c74-8f9f-4c7d-d1b4-e034f7b7b3be","pycharm":{"is_executing":true},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>comment_text_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>explanation\\n  edits made   username hardcore ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['explanation', 'edits', 'made', 'username', '...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>daww  matches  background colour im seemingly ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['daww', 'matches', 'background', 'colour', 'i...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>hey man im really  trying  edit war     guy  c...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['hey', 'man', 'im', 'really', 'trying', 'edit...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>cant make  real suggestions  improvement   won...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['cant', 'make', 'real', 'suggestions', 'impro...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>sir   hero  chance  remember  page thats</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  explanation\\n  edits made   username hardcore ...      0   \n","1  000103f0d9cfb60f  daww  matches  background colour im seemingly ...      0   \n","2  000113f07ec002fd  hey man im really  trying  edit war     guy  c...      0   \n","3  0001b41b1c6bb37e  cant make  real suggestions  improvement   won...      0   \n","4  0001d958c54c6e35           sir   hero  chance  remember  page thats      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \\\n","0             0        0       0       0              0   \n","1             0        0       0       0              0   \n","2             0        0       0       0              0   \n","3             0        0       0       0              0   \n","4             0        0       0       0              0   \n","\n","                                  comment_text_words  \n","0  ['explanation', 'edits', 'made', 'username', '...  \n","1  ['daww', 'matches', 'background', 'colour', 'i...  \n","2  ['hey', 'man', 'im', 'really', 'trying', 'edit...  \n","3  ['cant', 'make', 'real', 'suggestions', 'impro...  \n","4  ['sir', 'hero', 'chance', 'remember', 'page', ...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"/kaggle/input/dataset/train_cleaned.csv\")\n","\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenizing Comments using TextVectorization"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of characters in the longest comment is 5000\n","\n","Number of words in the longest comment is 456\n"]}],"source":["# finding the maximum number of words present in any given comment\n","maxlen = 0\n","longest_comment = \"\"\n","for comment in data['comment_text']:\n","    length = len(comment)\n","    if (length > maxlen):\n","        longest_comment = comment\n","    maxlen = max(maxlen, length)\n","\n","print(\"Number of characters in the longest comment is\", maxlen)\n","print(\"Number of words in the longest comment is\",\n","      len(longest_comment.split(\" \")) + 1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n","\n","    max_tokens=None,\n","\n","    # this is greater than the max words any comment has (774)\n","    # the remaning spots in the output would be padded by 0s\n","    output_sequence_length=800,\n","\n","    # converets to lowercase and skips all the punctuation\n","    standardize=\"lower_and_strip_punctuation\",\n","\n","    # the tokens will be split at whitespaces\n","    split=\"whitespace\",\n","\n","    # each of the tokens is represented as an integer\n","    output_mode=\"int\",\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["numpyArray = data[data.columns[1]].to_numpy()\n","vectorize_layer.adapt(numpyArray)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(800,), dtype=int64, numpy=\n","array([185, 161,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0], dtype=int64)>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# testing\n","vectorize_layer(\"hello, world!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Creating a Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["### Gets all the words into one string"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T19:53:21.215398Z","iopub.status.busy":"2023-02-26T19:53:21.214091Z","iopub.status.idle":"2023-02-26T19:53:21.229347Z","shell.execute_reply":"2023-02-26T19:53:21.228130Z","shell.execute_reply.started":"2023-02-26T19:53:21.215338Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1077\n","[[1], [2], [3], [1077]]\n"]}],"source":["# Creates the tokenizer class\n","tokenizer = keras.preprocessing.text.Tokenizer()\n","\n","# Combines all the words into one singular string\n","allWordsString = \" \".join(data.head(50)[\"comment_text\"].tolist())\n","allWordsList = allWordsString.split(r\"\\\\s+\")\n","\n","# Updates the tokenizer with the string of all words\n","tokenizer.fit_on_texts(allWordsList)\n","\n","# Prints word dictionary\n","# print(tokenizer.word_index)\n","\n","# Prints length of word dictionary\n","print(len(tokenizer.word_index))\n","\n","# Converts text to numbers\n","print(tokenizer.texts_to_sequences([\"page\", \"im\", \"use\", \"mussolini\"]))"]},{"cell_type":"markdown","metadata":{},"source":["# Text Vectorization Layer\n","### Following [https://www.tensorflow.org/text/tutorials/text_classification_rnn](http://)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T22:26:40.060442Z","iopub.status.busy":"2023-03-04T22:26:40.060049Z","iopub.status.idle":"2023-03-04T22:27:04.799663Z","shell.execute_reply":"2023-03-04T22:27:04.798502Z","shell.execute_reply.started":"2023-03-04T22:26:40.060408Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['', '[UNK]', 'article', 'page', 'wikipedia', 'talk', 'please',\n","       'would', 'one', 'like', 'dont', 'see', 'also', 'think', 'im',\n","       'know', 'people', 'edit', 'articles', 'use'], dtype='<U4955')"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["NUM_ROWS = 150000\n","\n","MAX_LENGTH = None\n","encoder = tf.keras.layers.TextVectorization(output_sequence_length=MAX_LENGTH)\n","encoder.adapt(data.head(NUM_ROWS)[\"comment_text\"].tolist())\n","\n","vocab = np.array(encoder.get_vocabulary())\n","vocab[:20]"]},{"cell_type":"markdown","metadata":{},"source":["### Testing the encoder\n","\n","Encoder removes punctuation and whitespace and forces lowercase so half the cleaning we did was useless"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-27T00:40:50.801014Z","iopub.status.busy":"2023-02-27T00:40:50.800600Z","iopub.status.idle":"2023-02-27T00:40:50.899632Z","shell.execute_reply":"2023-02-27T00:40:50.898088Z","shell.execute_reply.started":"2023-02-27T00:40:50.800979Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    explanation\\n  edits made   username hardcore ...\n","1    daww  matches  background colour im seemingly ...\n","2    hey man im really  trying  edit war     guy  c...\n","Name: comment_text, dtype: object\n","[[ 120   54   76  484 1020  867 1094  148  465  483 1273 1047  476  320\n","   438 1168 1104    7    8  104  258    5    2   67    3  697]\n"," [1211  885 1346 1267    3  661  263   29    5  955   90    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0]\n"," [ 355   75    3  106   62   13  472 1028  392  293  294   49  546   54\n","   976    5    2  284 1301  368 1421  342    0    0    0    0]]\n","explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired\n","daww matches background colour im seemingly stuck thanks talk january utc               \n","hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info    \n"]}],"source":["commentsToEncode = data.head(3)[\"comment_text\"]\n","print(commentsToEncode)\n","\n","encodedComments = encoder(commentsToEncode).numpy()\n","print(encodedComments)\n","\n","for comment in encodedComments:\n","    print(\" \".join(vocab[comment]))"]},{"cell_type":"markdown","metadata":{},"source":["### Building the model"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T22:27:04.802381Z","iopub.status.busy":"2023-03-04T22:27:04.801917Z","iopub.status.idle":"2023-03-04T22:27:16.679821Z","shell.execute_reply":"2023-03-04T22:27:16.678810Z","shell.execute_reply.started":"2023-03-04T22:27:04.802342Z"},"trusted":true},"outputs":[],"source":["# Sets random seed so results are identical every time\n","SEED = 1\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","regularization_layer = tf.keras.layers.Dense(\n","    64, \n","    activation=\"relu\", \n","    kernel_regularizer=tf.keras.regularizers.l1(0.01),\n","    activity_regularizer=tf.keras.regularizers.l2(0.01)\n",")\n","\n","model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(\n","        input_dim=len(encoder.get_vocabulary()),\n","        output_dim=512,\n","        mask_zero=True\n","    ),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation=\"relu\"),\n","    regularization_layer,\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","             optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Training the model"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T22:27:16.681929Z","iopub.status.busy":"2023-03-04T22:27:16.681466Z","iopub.status.idle":"2023-03-04T22:27:16.694441Z","shell.execute_reply":"2023-03-04T22:27:16.693503Z","shell.execute_reply.started":"2023-03-04T22:27:16.681885Z"},"trusted":true},"outputs":[],"source":["binaryDf = data.head(NUM_ROWS)[[\"comment_text\", \"toxic\"]]\n","\n","split_cutoff = int(0.8 * NUM_ROWS)\n","training_data = binaryDf.iloc[:split_cutoff]\n","validation_data = binaryDf.iloc[split_cutoff:]\n","\n","training_target = training_data.pop(\"toxic\")\n","validation_target = validation_data.pop(\"toxic\")"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T22:27:16.697511Z","iopub.status.busy":"2023-03-04T22:27:16.697093Z","iopub.status.idle":"2023-03-04T23:15:50.665758Z","shell.execute_reply":"2023-03-04T23:15:50.664515Z","shell.execute_reply.started":"2023-03-04T22:27:16.697475Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","3750/3750 [==============================] - 302s 77ms/step - loss: 0.1528 - accuracy: 0.9490 - val_loss: 0.1053 - val_accuracy: 0.9612\n","Epoch 2/10\n","3750/3750 [==============================] - 285s 76ms/step - loss: 0.0740 - accuracy: 0.9719 - val_loss: 0.1131 - val_accuracy: 0.9607\n","Epoch 3/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0467 - accuracy: 0.9822 - val_loss: 0.1334 - val_accuracy: 0.9581\n","Epoch 4/10\n","3750/3750 [==============================] - 285s 76ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 0.1687 - val_accuracy: 0.9560\n","Epoch 5/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.1853 - val_accuracy: 0.9532\n","Epoch 6/10\n","3750/3750 [==============================] - 285s 76ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.2211 - val_accuracy: 0.9494\n","Epoch 7/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.2465 - val_accuracy: 0.9487\n","Epoch 8/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.2654 - val_accuracy: 0.9446\n","Epoch 9/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.2880 - val_accuracy: 0.9452\n","Epoch 10/10\n","3750/3750 [==============================] - 286s 76ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.3137 - val_accuracy: 0.9493\n"]}],"source":["# # training_dataset = tf.convert_to_tensor(training_data)\n","# # validation_dataset = tf.convert_to_tensor(validation_data)\n","\n","# validation_target = validation_data.pop(\"toxic\")\n","# validation_dataset = tf.convert_to_tensor(validation_data)\n","\n","# # history = model.fit(training_dataset, target, epochs=10, validation_data=(validation_dataset, validation_target))\n","\n","# binaryDf = data.head(NUM_ROWS)[[\"comment_text\", \"toxic\"]]\n","# target = binaryDf.pop(\"toxic\")\n","# dataset = tf.convert_to_tensor(binaryDf)\n","\n","# Early Stopping\n","callback = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=2)\n","\n","history = model.fit(training_data, training_target, epochs=10, validation_data=(validation_data, validation_target), callbacks=[callback])"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T01:06:55.563202Z","iopub.status.busy":"2023-03-04T01:06:55.562502Z","iopub.status.idle":"2023-03-04T01:06:55.589164Z","shell.execute_reply":"2023-03-04T01:06:55.588291Z","shell.execute_reply.started":"2023-03-04T01:06:55.563096Z"},"trusted":true},"outputs":[],"source":["# sample_text = \"\"\n","# predictions = model.predict(np.array([sample_text]))\n","# predictions[0]"]},{"cell_type":"markdown","metadata":{},"source":["### Getting test data"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T22:02:45.030717Z","iopub.status.busy":"2023-03-04T22:02:45.030335Z","iopub.status.idle":"2023-03-04T22:02:46.182611Z","shell.execute_reply":"2023-03-04T22:02:46.181620Z","shell.execute_reply.started":"2023-03-04T22:02:45.030661Z"},"trusted":true},"outputs":[],"source":["test_data = pd.read_csv(\"/kaggle/input/toxic-competition-dataset/test.csv\")\n","test_labels = pd.read_csv(\"/kaggle/input/toxic-competition-dataset/test_labels.csv\")\n","\n","test_labels = test_labels.loc[test_labels[\"toxic\"] >= 0]\n","merged_df = test_labels.merge(test_data, left_on=\"id\", right_on=\"id\")"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T23:15:50.668655Z","iopub.status.busy":"2023-03-04T23:15:50.667673Z","iopub.status.idle":"2023-03-04T23:16:58.462435Z","shell.execute_reply":"2023-03-04T23:16:58.461394Z","shell.execute_reply.started":"2023-03-04T23:15:50.668616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2000/2000 [==============================] - 68s 34ms/step - loss: 1.4159 - accuracy: 0.6717\n"]},{"data":{"text/plain":["[1.4158982038497925, 0.6716840267181396]"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["# Tests all rows with a value of 0 or 1\n","\n","test_df = merged_df[[\"comment_text\", \"toxic\"]]\n","testTarget = test_df.pop(\"toxic\")\n","model.evaluate(test_df, testTarget)"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-03-04T23:16:58.465323Z","iopub.status.busy":"2023-03-04T23:16:58.464704Z","iopub.status.idle":"2023-03-04T23:17:04.151768Z","shell.execute_reply":"2023-03-04T23:17:04.150802Z","shell.execute_reply.started":"2023-03-04T23:16:58.465284Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["191/191 [==============================] - 6s 29ms/step - loss: 0.2665 - accuracy: 0.9212\n"]},{"data":{"text/plain":["[0.26647964119911194, 0.9211822748184204]"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["# Tests only rows with a toxic value of 1\n","\n","test_df = merged_df[[\"comment_text\", \"toxic\"]]\n","newTest_df = test_df.loc[test_df[\"toxic\"] == 1]\n","\n","newTestTarget = newTest_df.pop(\"toxic\")\n","model.evaluate(newTest_df, newTestTarget)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"ab65a6a126614c4d8a09c3bb162b3d2e4f4a949753c6f0f735c7c1fe269df83b"}}},"nbformat":4,"nbformat_minor":4}
